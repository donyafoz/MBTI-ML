{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MBTI Project\n",
    "\n",
    "By The Introverts "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our Workflow:\n",
    "----\n",
    "- Visualization\n",
    "- Data Preprocessing\n",
    "- Feature Engineering\n",
    "- Modeling\n",
    "- Evaluation on ROC plot\n",
    "- Prediction for Celebrity's type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reset -fs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.porter import *\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "from nltk import word_tokenize\n",
    "from sklearn.feature_extraction import stop_words\n",
    "from sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('fivethirtyeight')\n",
    "import seaborn as sns\n",
    "import string \n",
    "import re\n",
    "from collections import Counter\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import numpy as np\n",
    "from helperfunctions.PrettyConfusionMatrix import print_cm\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import StratifiedKFold, StratifiedShuffleSplit\n",
    "from sklearn.metrics import accuracy_score,roc_auc_score,f1_score,confusion_matrix, precision_recall_curve\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import time\n",
    "import warnings\n",
    "from collections import defaultdict\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "warnings.filterwarnings('ignore')\n",
    "from random import choice\n",
    "import lightgbm as lgb\n",
    "import gc\n",
    "from xgboost.sklearn import XGBClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mbti = pd.read_csv('data/mbti_1.csv') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization:\n",
    "#### Personality Types:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we take a look at distribution of the MBTI personality types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stat = mbti['type'].value_counts()\n",
    "plt.figure(figsize=(12,4))\n",
    "plt.bar(stat.index,stat.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4 capitals' meaning:\n",
    "- extroversion vs. introversion \n",
    "- intuition vs. sensing\n",
    "- thinking vs. feeling\n",
    "- judging vs. perceiving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Types per category\n",
    "# analyze types by sub-category may be easier\n",
    "mbti['EorI']= mbti['type'].apply(lambda x:x[0])\n",
    "mbti['NorS']= mbti['type'].apply(lambda x:x[1])\n",
    "mbti['TorF']= mbti['type'].apply(lambda x:x[2])\n",
    "mbti['JorP']= mbti['type'].apply(lambda x:x[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = sns.countplot(x='variable',hue='value',data=pd.melt(mbti.iloc[:,2:]),palette=\"Set2\")\n",
    "ax.set_xticklabels([\"introverted vs.extroverted\", \"intuition vs. sensing\", \n",
    "                    \"feeling vs.thinking \", \"judging vs. perceiving\", \n",
    "                    ], rotation=10, fontsize=11)\n",
    "ax.legend(loc='center left', bbox_to_anchor=(1, 0.5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imbalanced class: I vs E, N vs S. Some advanced techniques like resampling methods may be needed later to handle imbalanced classes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Their last 50 posts:\n",
    "Focus on some statistics(mean,variance) of posts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "words they averagely used in per comment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mbti['avg_comment_length'] = mbti['posts'].apply(lambda x: len(x.split())/50)\n",
    "mbti['comment_length_var'] = mbti['posts'].apply(lambda x: np.var([len(sentence.split()) for sentence in x.split('|||')]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mbti.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,6))\n",
    "sns.violinplot(x='type', y='avg_comment_length', data=mbti)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,6))\n",
    "sns.violinplot(x='type', y='comment_length_var', data=mbti)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define a function for cleaning the posts:\n",
    "- Remove \"|||\" and links and punctuations and MBTI words(As u see in the text, real names user referring to are changed to MBTI type)\n",
    "- Lowercase\n",
    "- Lemmatize word, remove stop words and words less than 3 characters (lemmatizing would be better than stemming but takes more time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get 16 types of personality list for removing MBTI words in the following function\n",
    "types = list(mbti.iloc[:,0].unique())\n",
    "types = [t.lower() for t in types]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def post_preprocess(df):\n",
    "    i = 0\n",
    "    post_list = []\n",
    "    length = len(df)\n",
    "    lemmatiser = WordNetLemmatizer() # or use PorterStemmer(), if want to run faster\n",
    "    print('Processing... Be patient')\n",
    "    \n",
    "    for row in df.iterrows():\n",
    "        # Progress bar\n",
    "        i+=1\n",
    "        if (i % 500 == 0 or i == length):\n",
    "            print(f\"Progress bar：{round(i/length*100)}%\")\n",
    "        # clean the posts\n",
    "        posts = row[1].posts\n",
    "        posts = re.sub(r'\\|\\|\\|',' ',posts)\n",
    "        posts = re.sub(r'http[\\S]*', '', posts).lower()\n",
    "        posts = re.sub(\"[^a-z\\s]\", ' ', posts)\n",
    "        posts = ' '.join([lemmatiser.lemmatize(w) for w in posts.split(' ') if w not in stopwords.words('english')])\n",
    "        for t in types:\n",
    "            posts = posts.replace(t,'')\n",
    "        post_list.append(posts)\n",
    "        \n",
    "    return np.array(post_list)      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here it takes 15mins to run\n",
    "#processed_post = post_preprocess(mbti)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for saving time, load the clean post we have done\n",
    "processed_post = pd.read_csv('data/mbti_preprocessed_1.csv')\n",
    "processed_post.drop('Unnamed: 0',axis=1,inplace=True)\n",
    "processed_post.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting our target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A type consists of 4 capitals and each capital corresponds to 2 possible characteristics.\n",
    "# For later encoding and modeling issues, let's tranfer them into separate binary code.\n",
    "type_map = {'I':0, 'E':1, 'N':0, 'S':1, 'F':0, 'T':1, 'J':0, 'P':1}\n",
    "types = {'EorI':'Extroversion vs. Introversion', 'NorS': 'Intuition vs. Sensing',\n",
    "                 'TorF': 'Thinking vs. Feeling','JorP': 'Judging vs. Perceiving'}\n",
    "# transfer column 3-6 into binary code.\n",
    "def type_preprocess(df):\n",
    "    for i in range(2,6):\n",
    "        df.iloc[:,i] = df.iloc[:,i].map(type_map)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mbti = type_preprocess(mbti)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mbti.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Vectorizing Posts "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a bag of words representation of each user by using tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TfidfVectorizer, those parameters are the  best for model performance.\n",
    "vectorizer_tfidf = TfidfVectorizer(min_df=0.05, max_df=0.85, analyzer='word', ngram_range=(1, 2))\n",
    "vectorizer_tfidf.fit(processed_post['processed_posts'])\n",
    "word_tfidf = vectorizer_tfidf.transform(processed_post['processed_posts'])\n",
    "word_tfidf_df = pd.DataFrame(data = word_tfidf.toarray(), columns = vectorizer_tfidf.get_feature_names())\n",
    "# CountVectorizer\n",
    "vectorizer_ct = CountVectorizer(stop_words='english',analyzer='word',input='content', \n",
    "                                 decode_error='ignore', max_df=0.48,min_df=5,\n",
    "                                 token_pattern=r'\\w{1,}', max_features=1625, ngram_range=(1,2)) # to compare two methods, I limit max_features=1625\n",
    "vectorizer_ct.fit(processed_post['processed_posts'])\n",
    "word_ct = vectorizer_ct.transform(processed_post['processed_posts'])\n",
    "word_ct_df = pd.DataFrame(data = word_ct.toarray(), columns = vectorizer_ct.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_tfidf_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_ct_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Manual Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sentiment Score "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Caution:** Takes a few minutes to run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Sentiment Score of clean post\n",
    "analyzer = SentimentIntensityAnalyzer()\n",
    "scores = []\n",
    "length_p = len(processed_post)\n",
    "for i in range(length_p):\n",
    "    score = analyzer.polarity_scores(processed_post['processed_posts'][i])['compound']\n",
    "    scores.append(score)\n",
    "    # Print Progress \n",
    "    if (i % 500 == 0 or i == length_p-1):\n",
    "            print(f\"Progress bar：{round(i/length_p*100)}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mbti['Sentiment'] = scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ellipses count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Counting number of ellipsies used per user."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a list of ellpsies count per user. This is an indicator for long posts \n",
    "ellipses_count = [len(re.findall(r'\\.\\.\\.\\|\\|\\|',posts)) for posts in mbti['posts']]\n",
    "# Append to dataset \n",
    "mbti['Ellipses'] = ellipses_count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exclamation count\n",
    "Counting number of exclamation marks used per user. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a list of exclamation count per user. \n",
    "exclamation_count = [len(re.findall(r'!',posts)) for posts in mbti['posts']]\n",
    "# Append to dataframe\n",
    "mbti['Exclamation'] = exclamation_count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question count\n",
    "Counting number of question marks used per user. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a list of question count per user. \n",
    "question_count = [len(re.findall(r'\\?',posts)) for posts in mbti['posts']]\n",
    "# Append to dataframe\n",
    "mbti['Question'] = question_count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Link count\n",
    "Counting number of links used per user. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For each user, remove ||| to make it easier to find links. \n",
    "user_posts = [re.sub(r'\\|\\|\\|',' ',posts) for posts in mbti['posts']]\n",
    "# Create a list of link count per user. \n",
    "link_count = [len(re.findall(r'http[\\S]* ', posts)) for posts in user_posts]\n",
    "# Append to dataframe\n",
    "mbti['Links'] = link_count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Picture count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a list of question count per user. \n",
    "question_count = [len(re.findall(r'(\\.png)|(\\.jpg)',posts)) for posts in mbti['posts']]\n",
    "# Append to dataframe\n",
    "mbti['Picture'] = question_count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Emojies count\n",
    "We should probably modify the text_style_emojies to include more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_emoji(text):\n",
    "    # REMOVE LATER ON\n",
    "    text = text.lower()\n",
    "    \n",
    "    text = re.sub(r'\\|\\|\\|', ' ', text)\n",
    "    \n",
    "    slack_style_emojies = re.findall(r':[\\w\\d]+(\\-[\\w\\d]+)?:', text)\n",
    "    text_style_emojies = re.findall(r':[\\-|\\s]?[d|\\)|\\(|p]', text)\n",
    "    \n",
    "    return slack_style_emojies + text_style_emojies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mbti['Emojies'] = mbti['posts'].map(lambda x: len(find_emoji(x)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Upper case count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def del_punct(text):\n",
    "    regex = re.compile('[' + re.escape(string.punctuation) + '\\\\r\\\\t\\\\n]')\n",
    "    return regex.sub(\"\", text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = mbti['posts'].apply(lambda x: del_punct(x))\n",
    "mbti['Upper'] = temp.apply(lambda x: len([x for x in x.split() if x.isupper()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "mbti.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Additional Cleaning\n",
    "- Naive Bayes doesn't take negative numbers.\n",
    "- Sentiment Scores have NaNs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NaNs were found: \n",
    "mbti.fillna(value=0, inplace=True)\n",
    "# Naive Bayes can't handle negatives? Scale with MinMax \n",
    "min_max_scaler = MinMaxScaler()\n",
    "sentiment_scaled = min_max_scaler.fit_transform(np.array(mbti['Sentiment']).reshape(-1, 1))\n",
    "mbti['Sentiment'] = sentiment_scaled"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Base Model: Multinomial NB & Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for saving time, load the data we have done in terms of feature engineering.\n",
    "mbti = pd.read_csv(\"data/mbti_FE.csv\")\n",
    "mbti.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we set a function to collect models' performances on roc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "\n",
    "def plot_confusion_matrix(cm, classes,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "\n",
    "    plt.imshow(cm, cmap=cmap) # interpolation changes the blurriness of the squares \n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "    plt.grid(b='False')\n",
    "\n",
    "    fmt = 'd'\n",
    "    thresh = cm.max() / 1.5 # threshold controls font color on opaque tile\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\") # if color is darker, use white \n",
    "\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_plot_data(probabilities):\n",
    "    '''\n",
    "    Calculates average ROC and Precision vs Recall curves over n_splits of stratified shuffle splits. \n",
    "    Not all thresholds are chosen equally, thus different splits have different array lengths of FPR, TPR,\n",
    "    precision, and recall. In order to average we need the same array length. We will interpolate tpr values \n",
    "    between 0 and 1 in 0.1 increments. This gives an average of a set of *estimated* ROC and Precision vs \n",
    "    recall estimates. \n",
    "\n",
    "    Function returns a dictionary with keys of class 'EorI', 'NorS', 'TorF', and 'JorP'.  \n",
    "    Within each class value contains another dictionary with keys 'base_x', 'est_tpr', 'roc_auc',\n",
    "    'est_pr', and 'auc_pr'. 'base_x' is the range of x values used to interpolate both the TPR and precision. \n",
    "    '''\n",
    "    model_data = defaultdict()\n",
    "    base_x = np.linspace(0,1,101)\n",
    "        \n",
    "    for types in probabilities.keys():\n",
    "        model_data[types] = {'base_x':base_x, 'est_tpr':np.zeros(101), 'auc_roc':[], 'est_pr':np.zeros(101),\n",
    "                             'auc_pr':[]} \n",
    "        total_splits = len(probabilities[types]) \n",
    "        for split in probabilities[types]:\n",
    "            y_scores = split[0] # split[0] is the model probability of predicting a 1\n",
    "            y_true = split[1] # split[1] is the true test values for that split\n",
    "            fpr, tpr, thresholds = roc_curve(y_true,y_scores) # used for interpolation\n",
    "            precision, recall, thresholds = precision_recall_curve(y_true, y_scores)\n",
    "            model_data[types]['est_tpr'] += np.interp(base_x, fpr, tpr) # Add est_tpr \n",
    "            model_data[types]['est_pr'] += np.interp(base_x, recall[::-1], precision[::-1]) # Add est_precision \n",
    "            model_data[types]['auc_roc'].append(auc(fpr, tpr)) # Append AUC \n",
    "            model_data[types]['auc_pr'].append(auc(recall, precision)) # Append AUC \n",
    "        model_data[types]['est_tpr'] = model_data[types]['est_tpr'] / total_splits # Average TPRs \n",
    "        model_data[types]['est_pr'] = model_data[types]['est_pr'] / total_splits # Average TPRs \n",
    "        model_data[types]['auc_roc'] = np.mean(model_data[types]['auc_roc']) # Average AUC-ROC\n",
    "        model_data[types]['auc_pr'] = np.mean(model_data[types]['auc_pr']) # Average PR\n",
    "        \n",
    "    return model_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def threshold_search(y_true, y_proba):\n",
    "    '''\n",
    "    searching a threshold to find the best f1-score\n",
    "    '''\n",
    "    best_threshold = 0\n",
    "    best_score = 0\n",
    "    for threshold in [i * 0.01 for i in range(100)]:\n",
    "        score = f1_score(y_true=y_true, y_pred=y_proba > threshold)\n",
    "        if score > best_score:\n",
    "            best_threshold = threshold\n",
    "            best_score = score\n",
    "#     search_result = {'threshold': best_threshold, 'f1': best_score} print if u want\n",
    "    return best_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(model, X, target, nsplits=4):\n",
    "    '''\n",
    "    Estimates probabilities for observations being in class 1.\n",
    "\n",
    "    Function feeds total probabilities into average ROC. Total probabilities is a dictionary \n",
    "    with keys of class 'EorI', 'NorS', 'TorF', and 'JorP'. Within each class value contains a \n",
    "    list of probabilities for each split.\n",
    "\n",
    "    Example: total_probabilities['EorI'] contains [[(p1,y1),(p2,y2),(p3,p3)...],[(p1,y1),(p2,y2),(p3,y3)...],...]\n",
    "    '''\n",
    "    \n",
    "    kf = StratifiedShuffleSplit(n_splits=nsplits, random_state=420)\n",
    "    model_data = defaultdict()\n",
    "\n",
    "    classes = {'EorI':['Extrovert','Introvert'], 'NorS':['Sensing', 'Intuition'],\n",
    "             'TorF':['Thinking','Feeling'],'JorP':['Perceiving','Judging']}\n",
    "\n",
    "    t = time.time()\n",
    "    for col in target.columns:\n",
    "        print(f\"* {classes[col][0]} vs. {classes[col][1]}\")\n",
    "        y = target[col]\n",
    "        all_auc = []\n",
    "        all_accuracies = []\n",
    "        f_score = []\n",
    "        model_data[col] = []\n",
    "        avg_cm = np.zeros(4).reshape(2,2).astype(int)\n",
    "        for train, test in kf.split(X, y):\n",
    "            X_train, X_test, y_train, y_test = X.loc[train], X.loc[test], y[train], y[test]\n",
    "            model.fit(X_train, y_train)\n",
    "            probabilities = model.predict_proba(X_test)\n",
    "            score = probabilities[:, 1]\n",
    "            preds = model.predict(X_test)\n",
    "            model_data[col].append((score, y_test))\n",
    "            all_auc.append(roc_auc_score(y_test,score))\n",
    "            fscore = threshold_search(y_test,score)\n",
    "            f_score.append(fscore)\n",
    "            avg_cm += confusion_matrix(y_test, preds,[1,0])\n",
    "        plt.figure()\n",
    "        plot_confusion_matrix(avg_cm//nsplits, classes=classes[col])\n",
    "        plt.show()\n",
    "        print(f'Average AUC: {np.mean(all_auc):.3f}; Average best fscore: {np.mean(f_score):.3f}')\n",
    "        print(\"\\n\")\n",
    "    print(f\"Time use:{time.time()-t:.3f}s\")\n",
    "\n",
    "    return get_plot_data(model_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize\n",
    "MNB = MultinomialNB()\n",
    "LR = LogisticRegression(random_state=0,class_weight='balanced')\n",
    "target = mbti.iloc[:,2:6]\n",
    "\n",
    "X_tf = pd.concat([mbti.iloc[:,6:],word_tfidf_df],axis=1)\n",
    "X_ct = pd.concat([mbti.iloc[:,6:],word_ct_df],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "mnb_ct_model = model(MNB, X_ct, target, nsplits=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnb_tf_model = model(MNB, X_tf, target, nsplits=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_tf_model = model(LR, X_tf, target, nsplits=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_ct_model = model(LR, X_ct, target, nsplits=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice the great gap between AUC and accuracy in EorI, IorS, both of which are unbalanced. But we can deal with it in the following models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RF = RandomForestClassifier(random_state=0, n_estimators=500, max_depth=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_ct_model = model(RF, X_ct, target, nsplits=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_tf_model = model(RF, X_tf, target, nsplits=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Advanced Model: Xgboost & Lightgbm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "XGB = XGBClassifier(eval_metric='auc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_ct_model = model(XGB, X_ct, target, nsplits=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "xgb_tf_model = model(XGB, X_tf, target, nsplits=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LightGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LGB = lgb.LGBMClassifier(eval_metric='auc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lgb_ct_model = model(LGB, X_ct, target, nsplits=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lgb_tf_model = model(LGB, X_tf, target, nsplits=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lightgbm performs a bit better than xgboost in terms of AUC and Accuracy and saves much more time. Besides, Lightgbm running on tfidf vectors outperforms a little bit. I decide to go further on lightgbm and dataset containing tfidf vectors(X_tf)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Search\n",
    "- iteration=20 takes nearly 25mins to run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kf = StratifiedShuffleSplit(n_splits=5, random_state=420)\n",
    "\n",
    "# loop for random search\n",
    "n_iterations=20\n",
    "\n",
    "print (\"Random search start...\")\n",
    "for col in target.columns:\n",
    "    print(f\"* {types[col]} \")\n",
    "    y = target[col]\n",
    "    roc_auc_mean = []\n",
    "    dict_list = []\n",
    "    for i in range(0, n_iterations):\n",
    "\n",
    "        param_dist = {'n_estimators' : choice([250,300,350,400,450]),\n",
    "                  'bagging_fraction': choice([0.5, 0.7, 0.8, 0.9]),\n",
    "                  'learning_rate': choice([0.05, 0.1, 0.3, 0.5]),\n",
    "                  'is_unbalance': True,\n",
    "                  'max_bin': choice([3, 5, 10, 15, 18, 20, 25]),\n",
    "                  'boosting_type' : choice(['gbdt', 'dart']),\n",
    "                  'max_depth': choice([2,3,4,5]),      \n",
    "                  'feature_fraction': choice([0.7, 0.8, 0.9]),\n",
    "                  'lambda_l1': choice([0, 10, 20, 30, 40]),\n",
    "                  'objective': 'binary', \n",
    "                  'metric': 'auc'} \n",
    "\n",
    "        roc_l = []\n",
    "\n",
    "        for train, test in kf.split(X_tf,y):\n",
    "\n",
    "            X_train, X_test, y_train, y_test = X_tf.loc[train], X_tf.loc[test], y[train], y[test]\n",
    "\n",
    "            # training\n",
    "            gbm = lgb.LGBMClassifier(**param_dist)\n",
    "            gbm.fit(X_train,y_train)\n",
    "            # predicting\n",
    "            y_pred = np.round(gbm.predict_proba(X_test)[:,1],3)\n",
    "            roc = roc_auc_score(y_test, y_pred)\n",
    "            roc_l.append(roc)\n",
    "\n",
    "        roc_array = np.asarray(roc_l)\n",
    "\n",
    "        roc_auc_mean.append(roc_array.mean())\n",
    "        dict_list.append(param_dist)\n",
    "        gc.collect()\n",
    "\n",
    "    results_pd = pd.DataFrame({\"roc_auc_mean\": roc_auc_mean,\"parameters\": dict_list})    \n",
    "\n",
    "    results_pd.sort_values(\"roc_auc_mean\", ascending = False, axis = 0, inplace = True)\n",
    "    top_pd = results_pd.head(1)\n",
    "    print(f\"--> Best AUC:{top_pd.iloc[0,0]} using {top_pd.iloc[0,1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def treemodel(X, target, nsplits=4):\n",
    "    '''\n",
    "    Estimates probabilities for observations being in class 1.\n",
    "\n",
    "    Function feeds total probabilities into average ROC. Total probabilities is a dictionary \n",
    "    with keys of class 'EorI', 'NorS', 'TorF', and 'JorP'. Within each class value contains a \n",
    "    list of probabilities for each split.\n",
    "\n",
    "    Example: total_probabilities['EorI'] contains [[p1,p2,p3...],[p1,p2,p3...],...]\n",
    "    '''\n",
    "\n",
    "    kf = StratifiedShuffleSplit(n_splits=nsplits, random_state=420)\n",
    "    total_probabilities = defaultdict()\n",
    "#     this is for lgb\n",
    "    para_list = [{'n_estimators': 400, 'bagging_fraction': 0.8, 'learning_rate': 0.1, 'is_unbalance': True, 'max_bin': 5, 'boosting_type': 'gbdt', 'max_depth': 3, 'feature_fraction': 0.7, 'lambda_l1': 40, 'objective': 'binary', 'metric': 'auc'},\n",
    "              {'n_estimators': 350, 'bagging_fraction': 0.5, 'learning_rate': 0.05, 'is_unbalance': True, 'max_bin': 15, 'boosting_type': 'gbdt', 'max_depth': 4, 'feature_fraction': 0.9, 'lambda_l1': 40, 'objective': 'binary', 'metric': 'auc'},\n",
    "              {'n_estimators': 450, 'bagging_fraction': 0.5, 'learning_rate': 0.1, 'is_unbalance': True, 'max_bin': 18, 'boosting_type': 'gbdt', 'max_depth': 3, 'feature_fraction': 0.9, 'lambda_l1': 20, 'objective': 'binary', 'metric': 'auc'},\n",
    "              {'n_estimators': 450, 'bagging_fraction': 0.8, 'learning_rate': 0.05, 'is_unbalance': True, 'max_bin': 5, 'boosting_type': 'gbdt', 'max_depth': 3, 'feature_fraction': 0.9, 'lambda_l1': 10, 'objective': 'binary', 'metric': 'auc'}]\n",
    "    # this is for rf\n",
    "#     para_list = [{'n_estimators': 1525, 'min_samples_split': 5, 'min_samples_leaf': 1, 'max_features': 'auto', 'max_depth': None, 'bootstrap': False},\n",
    "#              {'n_estimators': 575, 'min_samples_split': 10, 'min_samples_leaf': 1, 'max_features': 'auto', 'max_depth': 80, 'bootstrap': False},\n",
    "#              {'n_estimators': 1525, 'min_samples_split': 5, 'min_samples_leaf': 1, 'max_features': 'auto', 'max_depth': None, 'bootstrap': False},\n",
    "#              {'n_estimators': 100, 'min_samples_split': 2, 'min_samples_leaf': 4, 'max_features': 'auto', 'max_depth': 40, 'bootstrap': False}]\n",
    "    \n",
    "    classes = {'EorI':['Extroverts','Introverts'], 'NorS':['Sensing', 'Intuition'],\n",
    "             'TorF':['Thinking','Feeling'],'JorP':['Perceiving','Judging']}\n",
    "    \n",
    "    \n",
    "    t = time.time()\n",
    "    for i,col in enumerate(target.columns):\n",
    "        print(f\"* {types[col]} \")\n",
    "        param = para_list[i]\n",
    "        \n",
    "        y = target[col]\n",
    "        all_auc = []\n",
    "#         all_accuracies = []\n",
    "        f_score = []\n",
    "        total_probabilities[col] = []\n",
    "        avg_cm = np.zeros(4).reshape(2,2).astype(int)\n",
    "        for train, test in kf.split(X, y):\n",
    "            X_train, X_test, y_train, y_test = X.loc[train], X.loc[test], y[train], y[test] \n",
    "            gbm = lgb.LGBMClassifier(**param)\n",
    "            gbm.fit(X_train,y_train)\n",
    "#             rf = RandomForestClassifier(**param)\n",
    "#             rf.fit(X_train,y_train)\n",
    "            # predicting\n",
    "            probabilities = gbm.predict_proba(X_test)\n",
    "            preds = gbm.predict(X_test)\n",
    "#             probabilities = rf.predict_proba(X_test)\n",
    "            score = probabilities[:, 1]\n",
    "#             preds = model.predict(X_test)\n",
    "            total_probabilities[col].append((score, y_test))\n",
    "            all_auc.append(roc_auc_score(y_test,score))\n",
    "#             all_accuracies.append(accuracy_score(y_test,preds))\n",
    "            avg_cm += confusion_matrix(y_test, preds,[1,0])\n",
    "            fscore = threshold_search(y_test,score)\n",
    "            f_score.append(fscore)\n",
    "        plt.figure()\n",
    "        plot_confusion_matrix(avg_cm//nsplits, classes=classes[col])\n",
    "        plt.show()\n",
    "        \n",
    "        print(f'Average AUC: {np.mean(all_auc):.3f}; Average best fscore: {np.mean(f_score):.3f}')\n",
    "        print(\"\\n\")\n",
    "    print(f\"Time use:{time.time()-t:.3f}s\")\n",
    "\n",
    "    return get_plot_data(total_probabilities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lgb_tf_model_t = treemodel(X_tf, target, nsplits=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What about voting model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pm =  {'n_estimators': 450, 'bagging_fraction': 0.5, 'learning_rate': 0.1, 'is_unbalance': True, 'max_bin': 18, 'boosting_type': 'gbdt', 'max_depth': 3, 'feature_fraction': 0.9, 'lambda_l1': 20, 'objective': 'binary', 'metric': 'auc'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Lgbm = lgb.LGBMClassifier(**pm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import VotingClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "voting_clf = VotingClassifier(estimators = [('lgbm', Lgbm), \n",
    "                                            ('xgb', XGB), \n",
    "                                            ('mnb',MNB), \n",
    "                                            ('lr',LR)],\n",
    "                              voting = 'soft')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "voting = model(voting_clf, X_tf, target, nsplits=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation on ROC plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def auc_plot(model_list, model_names, plot_type):\n",
    "    '''\n",
    "    From model list and model names, plots ROC curves for each class on a 2x2 grid.\n",
    "    Add more colors to colors list if number of model exceeds current list length.\n",
    "    '''\n",
    "    fig, axs = plt.subplots(2,2, figsize=(15, 15), facecolor='w', edgecolor='b')\n",
    "    axs = axs.flatten()\n",
    "    abbrev = {'EorI':'Extroversion vs. Introversion', 'NorS': 'Intuition vs. Sensing',\n",
    "                     'TorF': 'Thinking vs. Feeling','JorP': 'Judging vs. Perceiving'}\n",
    "    colors = ['b','g','c','m','y']\n",
    "\n",
    "    if plot_type == 'roc':\n",
    "        y_axis = 'est_tpr'\n",
    "        auc_type = 'auc_roc'\n",
    "        plot_name = 'ROC'\n",
    "    elif plot_type =='pr':\n",
    "        y_axis = 'est_pr'\n",
    "        auc_type = 'auc_pr'\n",
    "        plot_name = 'Precision vs Recall'\n",
    "    else:\n",
    "        raise AttributeError('Invalid plot type')\n",
    "\n",
    "    for x in range(len(model_list)):\n",
    "        plot_data = model_list[x]\n",
    "        types = list(plot_data.keys())\n",
    "\n",
    "        for i in range(len(types)):\n",
    "            x_axis = plot_data[types[i]]['base_x']\n",
    "            est_y = plot_data[types[i]][y_axis]\n",
    "            auc = plot_data[types[i]][auc_type]\n",
    "            axs[i].plot(x_axis, est_y, colors[x], linewidth=1, label = '%s = %0.2f' %(model_names[x], auc))\n",
    "            if plot_type == 'roc':\n",
    "                axs[i].plot([0, 1], [0, 1],'r--', linewidth = 1)\n",
    "                axs[i].legend(loc = 'lower right', title = 'Avg AUC Scores',frameon=False)\n",
    "            else:\n",
    "                axs[i].legend(loc = 'upper right', title = 'Avg AUC Scores',frameon=False)            \n",
    "            axs[i].set_xlim([0, 1])\n",
    "            axs[i].set_ylim([0, 1])\n",
    "            axs[i].set_ylabel('True Positive Rate')\n",
    "            axs[i].set_xlabel('False Positive Rate')\n",
    "            axs[i].set_title(f'{plot_name} for {abbrev[types[i]]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_list = [lr_tf_model, lgb_tf_model_t, xgb_tf_model,rf_tf_model,voting] # 4 best models\n",
    "model_names = ['LR TFIDF', 'LGB TFIDF tuned','XGB TFIDF','RF TFIDF','VOTING TFIDF']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_list=[lr_ct_model, lr_tf_model, mnb_tf_model, mnb_ct_model]\n",
    "model_names=['test1','test2','test3','test4']\n",
    "plot_type='pr'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "auc_plot(model_list, model_names, plot_type='pr')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In terms of ROC, Logistic regression model performs the best."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction for celebrities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cb = pd.read_csv('data/celeb_twitter_mbti.csv')\n",
    "cb.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cb['EorI']= cb['type'].apply(lambda x:x[0])\n",
    "cb['NorS']= cb['type'].apply(lambda x:x[1])\n",
    "cb['TorF']= cb['type'].apply(lambda x:x[2])\n",
    "cb['JorP']= cb['type'].apply(lambda x:x[3])\n",
    "cb.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cb['avg_comment_length'] = cb['posts'].apply(lambda x: len(x.split())/50)\n",
    "cb['comment_length_var'] = cb['posts'].apply(lambda x: np.var([len(sentence.split()) for sentence in x.split('|||')]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "going on....."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Additional things to Consider:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- How to explain EorI things to the auidence in an easy way\n",
    "- add pca\n",
    "- others...."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
